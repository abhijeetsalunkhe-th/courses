<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Fine-Tuning - Modifying the Model</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Fine-Tuning: Modifying the Model</h1>

<h2>What is Fine-Tuning?</h2>
<p>Fine-tuning is the process of taking a pre-trained model and further training it on a smaller, specialized dataset. This process <strong>modifies the internal weights</strong> of the model, allowing it to internalize new styles, formats, or domain-specific behaviors.</p>

<div class="workflow-diagram">
    <div class="workflow-step">Pre-trained Model</div>
    <div class="workflow-arrow"></div>
    <div class="workflow-step">Specialized Dataset</div>
    <div class="workflow-arrow"></div>
    <div class="workflow-step">Training Process</div>
    <div class="workflow-arrow"></div>
    <div class="workflow-step">Fine-tuned Model</div>
</div>

<h2>Common Fine-Tuning Techniques</h2>
<ul>
    <li><strong>Full Fine-Tuning:</strong> Updating all parameters of the model. Requires significant compute resources.</li>
    <li><strong>Parameter-Efficient Fine-Tuning (PEFT):</strong> Techniques like <strong>LoRA</strong> (Low-Rank Adaptation) that only update a small fraction of the model's weights.</li>
    <li><strong>Instruction Tuning:</strong> Fine-tuning the model to follow specific types of instructions or response formats (e.g., JSON).</li>
</ul>

<h2>When Fine-Tuning is the Right Choice</h2>
<table>
    <tr><th>Use Case</th><th>Why Fine-Tuning?</th></tr>
    <tr><td class="rowheader">Brand Voice</td><td>Teaching the model to always respond in a very specific corporate tone.</td></tr>
    <tr><td class="rowheader">Specialized Output</td><td>Ensuring the model always outputs valid SQL or code for a proprietary framework.</td></tr>
    <tr><td class="rowheader">Niche Jargon</td><td>Helping the model understand industry-specific terms not found in general data.</td></tr>
    <tr><td class="rowheader">Instruction Following</td><td>Correcting persistent failures of a model to follow complex, multi-step instructions.</td></tr>
</table>

<h2>The Training Pipeline</h2>
<p>A typical fine-tuning pipeline involves several critical steps:</p>
<div class="code-block">
<pre><code># Typical Fine-Tuning Workflow
1. Data Collection & Curation (High-quality examples)
2. Data Formatting (e.g., Prompt/Completion pairs)
3. Model Selection (e.g., Llama-3, Mistral, GPT-4o-mini)
4. Hyperparameter Tuning (Learning rate, batch size)
5. Evaluation (Comparing against a test set)
6. Deployment (Hosting the specialized weights)</code></pre>
</div>

<h2>Key Limitations</h2>
<ul>
    <li><strong>Knowledge Cutoff:</strong> Fine-tuning 'bakes in' knowledge. To update that knowledge, you must retrain the model.</li>
    <li><strong>Hallucination Risk:</strong> While the model 'knows' the data, it can still hallucinate if the training data is inconsistent.</li>
    <li><strong>Cost:</strong> Upfront training costs can be high (GPU time + data prep).</li>
</ul>

<script type="text/javascript">
</script>
</body>
</html>
