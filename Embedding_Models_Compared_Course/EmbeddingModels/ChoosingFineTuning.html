<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Choosing and Fine-Tuning Embeddings</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Choosing and Fine-Tuning Embeddings</h1>

<h2>Selection Criteria for Enterprise RAG</h2>
<p>Selecting an embedding model goes beyond just looking at the top of the leaderboard. You must consider several factors specific to your application:</p>
<ul>
    <li><strong>Context Window:</strong> Does your data consist of short sentences or long technical documents? Voyage-3 supports 32K tokens, making it ideal for long-form content.</li>
    <li><strong>Multilingual Support:</strong> If you need to search across multiple languages, Cohere's embed-v3 is designed specifically for cross-lingual retrieval.</li>
    <li><strong>Cost at Scale:</strong> OpenAI's "small" model is highly cost-effective for high-volume applications, costing just $0.02 per 1M tokens.</li>
    <li><strong>Latency Requirements:</strong> Some models are faster than others. Self-hosting BGE models on local GPUs can eliminate network latency.</li>
</ul>

<h2>Matryoshka Representation Learning</h2>
<p>Modern models like OpenAI's `text-embedding-3-large` allow for truncation. This means you can generate a 3072-dimension vector but only store the first 256 or 512 dimensions. This dramatically reduces vector database costs while maintaining surprisingly high accuracy.</p>

<div class="code-block">
<pre><code># OpenAI Python Example: Truncating Embeddings
response = client.embeddings.create(
    model="text-embedding-3-large",
    input="The quick brown fox jumps over the lazy dog",
    dimensions=256  # Truncated from 3072
)
print(len(response.data[0].embedding)) # Output: 256</code></pre>
</div>

<h2>When to Fine-Tune Embeddings</h2>
<p>Most applications work well with "off-the-shelf" models. However, fine-tuning is necessary when:</p>
<ul>
    <li><strong>Domain-Specific Vocabulary:</strong> Your data uses terminology not found in general training sets (e.g., highly specific medical or legal jargon).</li>
    <li><strong>Unique Task Requirements:</strong> You need the model to rank documents based on a very specific internal logic.</li>
</ul>

<div class="code-block">
<pre><code># Fine-tuning BGE with Sentence-Transformers
from sentence_transformers import SentenceTransformer, losses, InputExample
from torch.utils.data import DataLoader

model = SentenceTransformer('BAAI/bge-small-en-v1.5')
train_examples = [
    InputExample(texts=['Query 1', 'Positive Doc 1']),
    InputExample(texts=['Query 2', 'Positive Doc 2'])
]
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.MultipleNegativesRankingLoss(model=model)

model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1)</code></pre>
</div>

<h2>Comparison Matrix: API vs. Open Source</h2>
<table>
    <tr><th>Feature</th><th>API (OpenAI/Cohere)</th><th>Open Source (BGE/Mistral)</th></tr>
    <tr><td class="rowheader">Ease of Use</td><td>High (Single API call)</td><td>Moderate (Requires infra)</td></tr>
    <tr><td class="rowheader">Cost</td><td>Variable (Per token)</td><td>Fixed (Infra/GPU cost)</td></tr>
    <tr><td class="rowheader">Privacy</td><td>Depends on TOS</td><td>Absolute (On-prem possible)</td></tr>
    <tr><td class="rowheader">Updates</td><td>Automatic</td><td>Manual redeployment</td></tr>
</table>

<script type="text/javascript">
</script>
</body>
</html>
