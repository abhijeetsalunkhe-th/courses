<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Embedding Fundamentals and Benchmarks</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Embedding Fundamentals and Benchmarks</h1>

<h2>What Are Text Embeddings?</h2>
<p>Text embeddings are dense vector representations of text that capture semantic meaning. Unlike traditional keyword matching, embeddings allow machines to understand the "context" and "intent" behind words. In a vector space, similar concepts are mathematically closer together.</p>

<div class="workflow-diagram">
    <div class="workflow-step">Raw Text Input</div>
    <div class="workflow-arrow"></div>
    <div class="workflow-step">Embedding Model</div>
    <div class="workflow-arrow"></div>
    <div class="workflow-step">Dense Vector</div>
    <div class="workflow-arrow"></div>
    <div class="workflow-step">Vector Database</div>
</div>

<h2>Core Technical Concepts</h2>
<ul>
    <li><strong>Vector Dimensions:</strong> The number of numerical values in the embedding. Common sizes are 768, 1536, and 3072.</li>
    <li><strong>Cosine Similarity:</strong> The mathematical formula used to calculate how similar two vectors are. It measures the angle between them in N-dimensional space.</li>
    <li><strong>Tokenization:</strong> Breaking down text into smaller units (tokens) before they are converted into embeddings.</li>
</ul>

<h2>Top Embedding Providers (2024-2025)</h2>
<table>
    <tr><th>Provider</th><th>Model</th><th>Dimensions</th><th>Max Tokens</th><th>Key Strength</th></tr>
    <tr><td>OpenAI</td><td>text-embedding-3-large</td><td>3072</td><td>8191</td><td>Matryoshka flexible dims</td></tr>
    <tr><td>Cohere</td><td>embed-v3.0</td><td>1024</td><td>512</td><td>Native Multilingual support</td></tr>
    <tr><td>Voyage AI</td><td>voyage-3</td><td>1024</td><td>32000</td><td>Optimized for long contexts</td></tr>
    <tr><td>BAAI (Open)</td><td>bge-large-en-v1.5</td><td>1024</td><td>512</td><td>Top-tier open source model</td></tr>
</table>

<h2>MTEB Benchmark: The Gold Standard</h2>
<p>The Massive Text Embedding Benchmark (MTEB) evaluates models across dozens of datasets and multiple tasks:</p>
<ul>
    <li><strong>Retrieval:</strong> Finding the most relevant documents for a query (measured by NDCG@10).</li>
    <li><strong>Classification:</strong> Categorizing text based on its embedding.</li>
    <li><strong>Clustering:</strong> Grouping similar documents together without labels.</li>
    <li><strong>STS:</strong> Semantic Textual Similarity scoring.</li>
</ul>

<h2>Dimension vs. Performance Trade-offs</h2>
<p>While higher dimensions generally provide better accuracy, they also increase storage costs and search latency. Using <strong>Matryoshka embeddings</strong> (supported by OpenAI) allows you to store fewer dimensions (e.g., 256 instead of 3072) while retaining ~95% of the retrieval performance.</p>

<script type="text/javascript">
</script>
</body>
</html>
